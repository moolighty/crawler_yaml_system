# 对应的爬虫名称，候选项通过scrapy list命令可以查看，定义位于src/spiders目录下
spider_name: universal

# 爬取网站的名称,url,描述
site_details:
    site_name: 作文网
    site_url: http://www.zuowen.com
    site_description: 作文网思想汇报抓取

# 1.允许的域名列表
allowed_domains:
    - www.zuowen.com

# 2.该网站spider自定义配置
custom_settings:
    # Pipelines指定,主要是对item提取出来之后,item的加工处理和流向,
    # 数字越小,优先级越大, 即item先流向优先级大的,处理后的item会流向次之优先级的pipeline
    ITEM_PIPELINES:
        src.pipelines.initialize_item.InitializeItemPipeline: 100
        src.pipelines.duplicate_filter_by_compute_id.DuplicatesFilterByComputeIdPipeline: 200
        src.pipelines.check_item_fields.CheckItemFieldsValidityPipeline: 300
        src.pipelines.save_item_to_file.SaveFileToLocaleByCategoryPipeline: 400
        # src.pipelines.save_item_to_kafka.SaveItemToKafkaPipeline: 500
    LOG_FILE : www.zuowen.com  # 保存爬虫运行情况的日志文件名称
    LOG_LEVEL: INFO  # 级别有DEBUG, INFO, WARNING, ERROR
    CONCURRENT_REQUESTS: 4
    CONCURRENT_REQUESTS_PER_DOMAIN: 4
    CONCURRENT_REQUESTS_PER_IP: 4

# 3.初始种子生成规则: 爬虫启动需要一批种子url
start_seeds_rules:
    detail_page_flag: false # 初始种子是否就是详情页,即产生最终结果item的页面, 默认置为为false
    check_before_request_flag: false  # 发出请求之前先判断是否之前请求过,避免重复请求，主要用来实现增量抓取
    depth: 0  # 初始请求的深度默认定位0
    request_factory_class: src.requestfactorys.DepthRequestFactory  # 主要用来生成对应的Reqeust对象
    category_urls:
        category_1: #类别种类命名理论上可以取任意名称，这里统一为category_n形式，n取值正整数
            - http://www.zuowen.com/e/20180726/5b59950749c85.shtml
            - http://www.zuowen.com/e/20180726/5b59928c8ee9c.shtml
            - http://www.zuowen.com/e/20180726/5b595a49460d0.shtml
    callback_urls: {}

# 4.用于生成request对象的params设置规则,主要涉及到请求头部参数，同时跟导航深度depth有关
request_params_rules:
    common_headers:
        Cookie: 'lcid=1033; aliyungf_tc=AQAAAM44QGfxoQMA/kW3Pf14qeDf4eVY; _ga=GA1.2.904588375.1557734558; _gid=GA1.2.1761012840.1557734558; Hm_lvt_05d39f4d0b6d45b03bf3bb358aba968a=1557734558; dialogClosed=1; Hm_lpvt_05d39f4d0b6d45b03bf3bb358aba968a=1557752099'
        Host: 'www.zuowen.com'
    depth_headers:
        - depths: [0, 1]
          params:
              method: GET
              headers: {}
              body: ''
              formdata: {}
              cookies: {}
              meta: {}
              dont_filter: false
              encoding: gbk

# 5.导航页链接和详情页多页时链接提取规则，用于生成request, 支持xml,html,json，re等方式提取,
#   根据depth和category来选择对应链接的提取方式
link_extractor_rules:
    # 导航页链接提取规则
    nav_link_settings:
        - depth: 1
          category: category_1 # 类别种类命名理论上可以取任意名称，这里统一为category_n形式，n取值正整数
          request_factory_class: src.requestfactorys.DepthRequestFactory
          meta:
              check_before_request_flag: true
              next_page_flag: false
              json_re: ''
              dont_filter: false   # 默认去重
          link_extractor_settings:
              link_extractor_class: scrapy.linkextractors.LinkExtractor  #src.misc.json_link_extractor.JsonLinkExtractor
              params:   # 对应类初始化对应的参数, 参数名称不能错
                  allow: .*\.shtml
                  restrict_xpaths: //div[@class="con_content"]/table/tbody/tr/td/a
          callback: parse_item
          cb_kwargs: {}
          follow:
          process_links:
          process_request:
    # 详情页下一页链接提取规则
    details_next_page_settings:
        next_page_flag: false
        merge_fields: [urls, body]
        extractors:
            - depth:
              request_factory_class: src.requestfactorys.DepthRequestFactory
              xpath: ''
              regex: ''

# 6.详情页item(即特征)提取规则, 根据category来选择相应特征的提取方式
item_extractor_rules:
    item_class: src.items.BaseItem
    item_loader_class: src.loaders.BaseItemLoader
    # 随着历史的变迁，一个网站可能有多种完全不同的解析方式
    # 元素格式:category_m => [case_1, case_2, ..., case_n]
    category_cases_relations:
        category_1: [case_1]
    extractor_type: 'xml'     # or "json", 根据此选择后面的提取器
    xml_field_extractors:
        case_1:
            # 一个字段可能有多种提取方式，所以是数组类型，下标大小决定优先级
            urls:
                - type: response_attr
                  param: url
                  regex: ''
            title:
                - type: response_meta_attr
                  param: link_text
                  regex: ''
            'organization_name':
                - type: value
                  param: '作文网'
                  regex: ''
            tags:
                - type: value
                  param: ['范文大全', '思想汇报']
                  regex: ''
            published_at:
                - type: xpath
                  param: '//div[@class="composition"]/p//text()'
                  regex: '(\d{4}-\d{2}-\d{2}\s*\d{2}:\d{2}:\d{2})'
            body:
                - type: xpath
                  param: '//div[@class="con_content"]'
                  regex: ''
            category:
                - type: xpath
                  param: '//div[@class="path"]/a/text()'
                  regex: ''
            email:
                - type: value
                  param: 'liangchuanjian@kingsoft.com'
                  regex: ''
    json_field_extractors:
        case_1:
            # 对应json path query language, 请参考jmepath库和scrapy.loader.processors.SelectJmes类
            json_path: '' # 'data[*].question.user.{urls: avatar_url, title: uname}'
            encoding: utf-8

# 7. item字段检查规则
item_field_check_rules:
    - callback: src.misc.field_checker.check_field_list_in_item
      params: [_id, urls, body]
    - callback: src.misc.field_checker.check_field_list_len_validity
      params:
          title: 1
          body: 50

# 8. item去重规则, 根据id去重
item_reduplicate_rules:
    reduplicate_flag: true
    id_compute_method: src.utils.coding_conversion.get_md5
    fields: [body]

# 9. item存入文件的规则
item_file_rules:
    parent_dir: ''
    file_suffix: '.json'
